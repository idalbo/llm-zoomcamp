{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac92a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7db63a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee8bfe",
   "metadata": {},
   "source": [
    "### Getting the raw data from repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b8fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdaeabc",
   "metadata": {},
   "source": [
    "### Index the raw data with elasticsearch\n",
    "\n",
    "Exercices 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5201791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticsearch build hash is: dbcbbbd0bc4924cfeb28929dc05d82d662c527b7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da793a84177c428684f581292b1d4efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# getting hash version of elasticsearch client\n",
    "print(f\"Elasticsearch build hash is: {es_client.info()[\"version\"][\"build_hash\"]}\")\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)\n",
    "\n",
    "# we use the `index` method from the Elasticsearch client to index each document\n",
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10cef6",
   "metadata": {},
   "source": [
    "### Execute query against the indexed data\n",
    "\n",
    "Exercices 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b5d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_elasticsearch(index_name, query, fields, search_type, result_size=5, course=None):\n",
    "    search_query = {\n",
    "        \"size\": result_size,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query,\n",
    "                            \"fields\": fields,\n",
    "                            \"type\": search_type\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                # Only add filter if course is provided\n",
    "                **(\n",
    "                    {\"filter\": [{\"term\": {\"course\": course}}]}\n",
    "                    if course else {}\n",
    "                )\n",
    "            }\n",
    "        }\n",
    "    }  \n",
    "    \n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f023d1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of kubernetes first hit is: 42.910965\n",
      "Third hit of the question about docker is: How do I copy files from a different folder into docker containerâ€™s working directory?\n"
     ]
    }
   ],
   "source": [
    "query_k8s = \"How do execute a command on a Kubernetes pod?\"\n",
    "\n",
    "response_k8s = search_elasticsearch(\n",
    "    index_name=index_name,\n",
    "    query=query_k8s,\n",
    "    fields=[\"question^4\", \"text\"],\n",
    "    search_type=\"best_fields\",\n",
    ")\n",
    "\n",
    "# print score of the first hit of the the first question aboud kubernetes\n",
    "print(f\"Score of kubernetes first hit is: {response_k8s[\"hits\"][\"hits\"][0][\"_score\"]}\")\n",
    "\n",
    "query_docker =  \"How do copy a file to a Docker container?\"\n",
    "\n",
    "response_docker = search_elasticsearch(\n",
    "    index_name=index_name,\n",
    "    query=query_docker,\n",
    "    fields=[\"question^4\", \"text\"],\n",
    "    search_type=\"best_fields\",\n",
    "    course=\"machine-learning-zoomcamp\",\n",
    "    result_size=3\n",
    ")\n",
    "\n",
    "# print the third hit of the question about docker\n",
    "print(f\"Third hit of the question about docker is: {response_docker['hits']['hits'][2]['_source'][\"question\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a988495",
   "metadata": {},
   "source": [
    "### Building a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7233f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    context_template = \"\"\"\n",
    "Q: {question}\n",
    "A: {text}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + context_template.format(question=doc[\"question\"], text=doc[\"text\"]) +\"\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f1dda5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the prompt is: 1446 characters\n"
     ]
    }
   ],
   "source": [
    "response_docker_docs = []\n",
    "\n",
    "for hit in response_docker['hits']['hits']:\n",
    "    response_docker_docs.append(hit['_source'])\n",
    "\n",
    "prompt_docker = build_prompt(query_docker, response_docker_docs)\n",
    "\n",
    "# print the length of the prompt\n",
    "print(f\"Length of the prompt is: {len(prompt_docker)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ae482",
   "metadata": {},
   "source": [
    "### Tokens in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09c9970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the prompt is: 320 tokens\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "# calculate the number of tokens in the prompt\n",
    "print(f\"Number of tokens in the prompt is: {len(encoding.encode(prompt_docker))} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99278b2c",
   "metadata": {},
   "source": [
    "### Send the prompt to the LLM - Groq - Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bb917b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "groq_response = llm(prompt_docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16796e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To copy a file to a Docker container, you can use the `docker cp` command. The basic syntax is as follows: \n",
      "`docker cp /path/to/local/file_or_directory container_id:/path/in/container` \n",
      "\n",
      "You need to replace `/path/to/local/file_or_directory` with the path of the file on your local machine, `container_id` with the ID of the Docker container, and `/path/in/container` with the path where you want to copy the file in the container.\n"
     ]
    }
   ],
   "source": [
    "print(groq_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8cb241",
   "metadata": {},
   "source": [
    "### Calculate the cost of requests - bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce288b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost per 1000 requests given an average of 150 sent tokens and an average of 250 received token per request, is: $2.300000\n"
     ]
    }
   ],
   "source": [
    "sent_token = 150\n",
    "received_token = 250\n",
    "\n",
    "cost_input_per_token = 2 / 1000000\n",
    "cost_per_output_token = 8 / 1000000\n",
    "\n",
    "cost_per_1000_request = ((sent_token * cost_input_per_token) + (received_token * cost_per_output_token)) * 1000\n",
    "\n",
    "print(f\"Cost per 1000 requests given an average of {sent_token} sent tokens and an average of {received_token} received token per request, is: ${cost_per_1000_request:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
